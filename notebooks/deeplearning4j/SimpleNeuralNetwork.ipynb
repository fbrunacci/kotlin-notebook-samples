{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%use deeplearning4j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple network from scrach and with Deeplearning4J\n",
    "Inspired by \n",
    "- https://www.youtube.com/watch?v=kft1AJ9WVDk\n",
    "- https://www.youtube.com/watch?v=SGZ6BttHMPw&list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH\n",
    "- [deeplearning4j-examples:MultiClassLogit.java](https://github.com/eclipse/deeplearning4j-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/dataexamples/MultiClassLogit.java)\n",
    "\n",
    "### Prediction of an binary computation\n",
    "\n",
    "#### Probleme set\n",
    "\n",
    "Given some training data:\n",
    "\n",
    "|           |  inputs ||| outputs |\n",
    "|-----------|---|---|---|---|\n",
    "| example 1 | 0 | 0 | 1 | 0 |\n",
    "| example 2 | 1 | 1 | 1 | 1 |\n",
    "| example 3 | 1 | 0 | 1 | 1 |\n",
    "| example 4 | 0 | 1 | 1 | 0 |\n",
    "\n",
    "What should the new output be for\n",
    "\n",
    "|           |  inputs ||| outputs |\n",
    "|-----------|---|---|---|---|\n",
    "| new situation | 1 | 1 | 1 | ? |\n",
    "\n",
    "Using a simple neural network\n",
    "\n",
    "![simple neural network](./images/simple_neuron_classification.svg \"simple neural network\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate inputs / outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "val training_inputs = Nd4j.create(arrayOf(\n",
    "    floatArrayOf(0f, 0f, 1f), \n",
    "    floatArrayOf(1f, 1f, 1f), \n",
    "    floatArrayOf(1f, 0f, 1f), \n",
    "    floatArrayOf(0f, 1f, 1f)))\n",
    "\n",
    "val training_outputs = Nd4j.create(arrayOf(\n",
    "    floatArrayOf(0f), \n",
    "    floatArrayOf(1f), \n",
    "    floatArrayOf(1f), \n",
    "    floatArrayOf(0f)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Starting from scrach\n",
    "### Artificial neuron in action\n",
    "\n",
    "<img align=\"right\" width=\"400\" src=\"./images/artificial_neuron.svg\">\n",
    "An articial neuron is a computational unit which will make a particular computation based on ohter units it's connected to.\n",
    "\n",
    "- **x** : input\n",
    "- **w** : connection weight\n",
    "- **b** : neuron bias\n",
    "- **g(.)** : activation function\n",
    "\n",
    "<br/>\n",
    "- Neuron pre-activation (or input activation):\n",
    "\\begin{equation*}\n",
    "pa(x) = b + \\sum_{n=1}^{d} w_i x_i \n",
    "\\end{equation*}\n",
    "- Neuron (output) activation:\n",
    "\\begin{equation*}\n",
    "h(x) = g(pa(x)) = g(b + \\sum_{n=1}^{d} w_i x_i)\n",
    "\\end{equation*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.nd4j.linalg.ops.transforms.Transforms\n",
    "\n",
    "interface ActivationFunction {\n",
    "    operator fun invoke(a: INDArray): INDArray\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun pa(x: INDArray, w: INDArray, b: INDArray): INDArray {\n",
    "    return b.add(x.mmul(w)) // matrix multiplication (apply weights for each x)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun h(x: INDArray, w: INDArray, g: ActivationFunction, b: INDArray): INDArray {\n",
    "    return g(pa(x,w,b)) // activation function\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Function\n",
    "\n",
    "In artificial neural networks, the activation function of a node defines the output of that node given an input or set of inputs. ([wikipedia](https://en.wikipedia.org/wiki/Activation_function))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some activation functions:\n",
    "\n",
    "<table align=\"left\" border=\"0\">\n",
    "<tr>\n",
    "    <td>Identity</td>\n",
    "    <td><img src=\"./images/Activation_identity.svg.png\"></td>\n",
    "    <td>\\begin{equation*}g(a) = a\\end{equation*}</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>Sigmoid</td>\n",
    "    <td><img src=\"./images/Activation_logistic.svg.png\"></td>\n",
    "    <td>\\begin{equation*}sigmoid(a) = \\frac{\\mathrm{1} }{\\mathrm{1} + e^a }\\end{equation*}</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>TanH</td>\n",
    "    <td><img src=\"./images/Activation_tanh.svg.png\"></td>\n",
    "    <td>\\begin{equation*}tanh(a) = \\frac{e^ a - e^{-a} }{e^a + e^{-a} }\\end{equation*}</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>ReLU</td>\n",
    "    <td><img src=\"./images/Activation_rectified_linear.svg.png\"></td>\n",
    "    <td>\\begin{equation*}relu(a) = max(0,a)\\end{equation*}</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid : ActivationFunction {\n",
    "    override operator fun invoke(a: INDArray): INDArray {\n",
    "        var a = a\n",
    "        a = a.mul(-1.0)\n",
    "        a = Transforms.exp(a, false)\n",
    "        a = a.add(1.0)\n",
    "        a = a.rdiv(1.0)\n",
    "        return a\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update neuron parameters (weights and bias)\n",
    "\n",
    "Once activation function is computed we need to update weights and bias parameters\n",
    "\n",
    "#### Using stochastic gradient descent (SGD) \n",
    "<img align=\"right\" width=\"400\" src=\"./images/artificial_neuron_backprop.svg\">\n",
    "\n",
    "Iterative method for optimizing an objective function by performing updates after each example. It can be regarded as a stochastic approximation of [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent).\n",
    "\n",
    "<img align=\"right\" width=\"330\" src=\"./images/gradient_descent.svg\">\n",
    "\n",
    "Gradient descent is an iterative algorithm, that starts from a random point on a function and travels down its slope in steps until it reaches the lowest point of that function.\n",
    "\n",
    "#### gradient descent:\n",
    "\n",
    "\\begin{equation*}\n",
    "  gradiant() = (Å·-y)x\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "  w_{t+1} = w_t - learning\\_rate * gradiant()\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun gradient(x: INDArray, y: INDArray,g: ActivationFunction, \n",
    "             weights: INDArray, bias: INDArray): INDArray {\n",
    "    val m = x.size(0) //number of examples\n",
    "    val pred = h(x, weights, g, bias) // prediction of the current network \n",
    "    val diff = pred.dup().sub(y) //diff between predicted and expected\n",
    "    return x.dup()\n",
    "            .transpose()\n",
    "            .mmul(diff)\n",
    "            //.mul(1.0 / m) // regularization \n",
    "}    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm that performs updates after each example (training)\n",
    "\n",
    "##### training function:\n",
    " - for nIterations \n",
    " - compute gradient\n",
    " - apply learningRate\n",
    " - propagate gradient to weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nd4j.getRandom().setSeed(1234)\n",
    "\n",
    "val learningRate = 0.9.toDouble()\n",
    "val nIterations = 10000\n",
    "val bias = Nd4j.zeros(1,1) // no bias\n",
    "val g = Sigmoid()\n",
    "\n",
    "fun training(g: ActivationFunction, x: INDArray, y: INDArray, maxIterations: Int): INDArray {\n",
    "    \n",
    "    var weights = Nd4j.rand(x.size(1).toInt(), 1) // init random weights\n",
    "\n",
    "    for (i in 0 until maxIterations) {\n",
    "        var gradients = gradient(x, y, g, weights, bias)\n",
    "        gradients = gradients.mul(learningRate)\n",
    "        weights = weights.sub(gradients)\n",
    "    }\n",
    "    return weights\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train the data and retrieve the optimized_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "val optimized_weights = training(g, training_inputs, training_outputs, nIterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### compute using the optimized weights applied to the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun compute_neural_network(x1: Number, x2: Number, x3: Number) {\n",
    "    val inputs = Nd4j.create(arrayOf(floatArrayOf(x1.toFloat(), x2.toFloat(), x3.toFloat())))\n",
    "    val output = h(inputs, optimized_weights, g, bias)\n",
    "    println(\"input: \"+ x1 +\", \"+ x2 +\", \"+ x3 +\" | output = \"+ output)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 0, 0, 1 | output = [[0.0002]]\n",
      "input: 1, 1, 1 | output = [[0.9999]]\n",
      "input: 1, 0, 1 | output = [[0.9999]]\n",
      "input: 0, 1, 1 | output = [[0.0001]]\n",
      "Prediction:\n",
      "input: 1, 1, 1 | output = [[0.9999]]\n"
     ]
    }
   ],
   "source": [
    "compute_neural_network(0,0,1)\n",
    "compute_neural_network(1,1,1)\n",
    "compute_neural_network(1,0,1)\n",
    "compute_neural_network(0,1,1)\n",
    "println(\"Prediction:\")\n",
    "compute_neural_network(1,1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loss func\n",
    "\n",
    "loss function is a method of evaluating how well specific algorithm models the given data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun loss(oldParams: INDArray, newParams: INDArray): Double {\n",
    "        val diffSum: Double = Transforms.abs(oldParams.sub(newParams)).sumNumber().toDouble()\n",
    "        return diffSum / oldParams.size(0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun training_with_loss(g: ActivationFunction, x: INDArray, y: INDArray, maxIterations: Int): INDArray {\n",
    "    \n",
    "    var weights = Nd4j.rand(x.size(1).toInt(), 1) // init random weights\n",
    "\n",
    "    for (i in 0 until maxIterations) {\n",
    "        var gradients = gradient(x, y, g, weights, bias)\n",
    "        gradients = gradients.mul(learningRate)\n",
    "        val newWeights = weights.sub(gradients)\n",
    "        println(\"Iteration \" + (i+1) + \" - loss: \" + loss(weights, newWeights))\n",
    "        weights = weights.sub(gradients)\n",
    "    }\n",
    "    return weights\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 - loss: 0.5761516888936361\n",
      "Iteration 2 - loss: 0.23400044441223145\n",
      "Iteration 3 - loss: 0.23013293743133545\n",
      "Iteration 4 - loss: 0.18769816557566324\n",
      "Iteration 5 - loss: 0.16302559773127237\n",
      "Iteration 6 - loss: 0.14307681719462076\n",
      "Iteration 7 - loss: 0.12719833850860596\n",
      "Iteration 8 - loss: 0.11430616180102031\n",
      "Iteration 9 - loss: 0.10366164644559224\n",
      "Iteration 10 - loss: 0.09474410613377889\n",
      "Iteration 11 - loss: 0.08717785278956096\n",
      "Iteration 12 - loss: 0.0806858241558075\n",
      "Iteration 13 - loss: 0.07512431343396504\n",
      "Iteration 14 - loss: 0.0703667402267456\n",
      "Iteration 15 - loss: 0.06614719827969869\n",
      "Iteration 16 - loss: 0.06238238016764323\n",
      "Iteration 17 - loss: 0.059005339940389\n",
      "Iteration 18 - loss: 0.055960655212402344\n",
      "Iteration 19 - loss: 0.05320302645365397\n",
      "Iteration 20 - loss: 0.050694803396860756\n",
      "Iteration 21 - loss: 0.04840462406476339\n",
      "Iteration 22 - loss: 0.04630579551060995\n",
      "Iteration 23 - loss: 0.044375727574030556\n",
      "Iteration 24 - loss: 0.0425956646601359\n",
      "Iteration 25 - loss: 0.040948679049809776\n",
      "Iteration 26 - loss: 0.039421031872431435\n",
      "Iteration 27 - loss: 0.03800023595492045\n",
      "Iteration 28 - loss: 0.03667585055033366\n",
      "Iteration 29 - loss: 0.0354384183883667\n",
      "Iteration 30 - loss: 0.03427975376447042\n",
      "Iteration 31 - loss: 0.03319286306699117\n",
      "Iteration 32 - loss: 0.03217121958732605\n",
      "Iteration 33 - loss: 0.031209369500478108\n",
      "Iteration 34 - loss: 0.03030205766359965\n",
      "Iteration 35 - loss: 0.029445042212804157\n",
      "Iteration 36 - loss: 0.028634339570999146\n",
      "Iteration 37 - loss: 0.027866214513778687\n",
      "Iteration 38 - loss: 0.02713744839032491\n",
      "Iteration 39 - loss: 0.026445160309473675\n",
      "Iteration 40 - loss: 0.02578665812810262\n",
      "Iteration 41 - loss: 0.02515960733095805\n",
      "Iteration 42 - loss: 0.024561971426010132\n",
      "Iteration 43 - loss: 0.02399138609568278\n",
      "Iteration 44 - loss: 0.02344653010368347\n",
      "Iteration 45 - loss: 0.022925257682800293\n",
      "Iteration 46 - loss: 0.0224265456199646\n",
      "Iteration 47 - loss: 0.02194856603940328\n",
      "Iteration 48 - loss: 0.021490365266799927\n",
      "Iteration 49 - loss: 0.021050751209259033\n",
      "Iteration 50 - loss: 0.020628223816553753\n",
      "Iteration 51 - loss: 0.020222375790278118\n",
      "Iteration 52 - loss: 0.019831915696461994\n",
      "Iteration 53 - loss: 0.019455949465433758\n",
      "Iteration 54 - loss: 0.019093811511993408\n",
      "Iteration 55 - loss: 0.01874483625094096\n",
      "Iteration 56 - loss: 0.01840813954671224\n",
      "Iteration 57 - loss: 0.018083244562149048\n",
      "Iteration 58 - loss: 0.017769416173299152\n",
      "Iteration 59 - loss: 0.017466137806574505\n",
      "Iteration 60 - loss: 0.01717299222946167\n",
      "Iteration 61 - loss: 0.016889373461405437\n",
      "Iteration 62 - loss: 0.01661492387453715\n",
      "Iteration 63 - loss: 0.016349246104558308\n",
      "Iteration 64 - loss: 0.01609179377555847\n",
      "Iteration 65 - loss: 0.015842139720916748\n",
      "Iteration 66 - loss: 0.015600005785624186\n",
      "Iteration 67 - loss: 0.015365183353424072\n",
      "Iteration 68 - loss: 0.015137344598770142\n",
      "Iteration 69 - loss: 0.01491585373878479\n",
      "Iteration 70 - loss: 0.014700790246327719\n",
      "Iteration 71 - loss: 0.01449186603228251\n",
      "Iteration 72 - loss: 0.014288634061813354\n",
      "Iteration 73 - loss: 0.014091203610102335\n",
      "Iteration 74 - loss: 0.013898849487304688\n",
      "Iteration 75 - loss: 0.013711621363957724\n",
      "Iteration 76 - loss: 0.013529459635416666\n",
      "Iteration 77 - loss: 0.013352145751317343\n",
      "Iteration 78 - loss: 0.013178954521814982\n",
      "Iteration 79 - loss: 0.013010720411936441\n",
      "Iteration 80 - loss: 0.0128461221853892\n",
      "Iteration 81 - loss: 0.01268590490023295\n",
      "Iteration 82 - loss: 0.012529492378234863\n",
      "Iteration 83 - loss: 0.012376964092254639\n",
      "Iteration 84 - loss: 0.012228240569432577\n",
      "Iteration 85 - loss: 0.012082626422246298\n",
      "Iteration 86 - loss: 0.011940727631251017\n",
      "Iteration 87 - loss: 0.01180189847946167\n",
      "Iteration 88 - loss: 0.011666357517242432\n",
      "Iteration 89 - loss: 0.011533786853154501\n",
      "Iteration 90 - loss: 0.011404424905776978\n",
      "Iteration 91 - loss: 0.011277695496877035\n",
      "Iteration 92 - loss: 0.01115378737449646\n",
      "Iteration 93 - loss: 0.011032481988271078\n",
      "Iteration 94 - loss: 0.01091356078783671\n",
      "Iteration 95 - loss: 0.010797441005706787\n",
      "Iteration 96 - loss: 0.010683735211690268\n",
      "Iteration 97 - loss: 0.010572393735249838\n",
      "Iteration 98 - loss: 0.01046315828959147\n",
      "Iteration 99 - loss: 0.010356237490971884\n",
      "Iteration 100 - loss: 0.010251730680465698\n",
      "Iteration 101 - loss: 0.010148833195368448\n",
      "Iteration 102 - loss: 0.01004826029141744\n",
      "Iteration 103 - loss: 0.009949594736099243\n",
      "Iteration 104 - loss: 0.009852886199951172\n",
      "Iteration 105 - loss: 0.009758015473683676\n",
      "Iteration 106 - loss: 0.009664803743362427\n",
      "Iteration 107 - loss: 0.00957345962524414\n",
      "Iteration 108 - loss: 0.009483933448791504\n",
      "Iteration 109 - loss: 0.009395956993103027\n",
      "Iteration 110 - loss: 0.009309510389963785\n",
      "Iteration 111 - loss: 0.009224524100621542\n",
      "Iteration 112 - loss: 0.009141365687052408\n",
      "Iteration 113 - loss: 0.009059548377990723\n",
      "Iteration 114 - loss: 0.008978992700576782\n",
      "Iteration 115 - loss: 0.00890024503072103\n",
      "Iteration 116 - loss: 0.008822441101074219\n",
      "Iteration 117 - loss: 0.008746077617009481\n",
      "Iteration 118 - loss: 0.00867117444674174\n",
      "Iteration 119 - loss: 0.008597354094187418\n",
      "Iteration 120 - loss: 0.008524854977925619\n",
      "Iteration 121 - loss: 0.00845375657081604\n",
      "Iteration 122 - loss: 0.008383840322494507\n",
      "Iteration 123 - loss: 0.008314689000447592\n",
      "Iteration 124 - loss: 0.008246809244155884\n",
      "Iteration 125 - loss: 0.008180071910222372\n",
      "Iteration 126 - loss: 0.008114437262217203\n",
      "Iteration 127 - loss: 0.00804978609085083\n",
      "Iteration 128 - loss: 0.007986396551132202\n",
      "Iteration 129 - loss: 0.007923652728398642\n",
      "Iteration 130 - loss: 0.007861912250518799\n",
      "Iteration 131 - loss: 0.00780107577641805\n",
      "Iteration 132 - loss: 0.0077415406703948975\n",
      "Iteration 133 - loss: 0.0076826512813568115\n",
      "Iteration 134 - loss: 0.007624735434850057\n",
      "Iteration 135 - loss: 0.0075674355030059814\n",
      "Iteration 136 - loss: 0.0075112879276275635\n",
      "Iteration 137 - loss: 0.007455895344416301\n",
      "Iteration 138 - loss: 0.0074008603890736895\n",
      "Iteration 139 - loss: 0.0073472559452056885\n",
      "Iteration 140 - loss: 0.007294128338495891\n",
      "Iteration 141 - loss: 0.007241666316986084\n",
      "Iteration 142 - loss: 0.007190148035685222\n",
      "Iteration 143 - loss: 0.007139285405476888\n",
      "Iteration 144 - loss: 0.00708917776743571\n",
      "Iteration 145 - loss: 0.007039904594421387\n",
      "Iteration 146 - loss: 0.006991138060887654\n",
      "Iteration 147 - loss: 0.006942977507909139\n",
      "Iteration 148 - loss: 0.006895283857981364\n",
      "Iteration 149 - loss: 0.006848762432734172\n",
      "Iteration 150 - loss: 0.006802519162495931\n",
      "Iteration 151 - loss: 0.00675693154335022\n",
      "Iteration 152 - loss: 0.00671196977297465\n",
      "Iteration 153 - loss: 0.006667663653691609\n",
      "Iteration 154 - loss: 0.006623675425847371\n",
      "Iteration 155 - loss: 0.006580660740534465\n",
      "Iteration 156 - loss: 0.006537795066833496\n",
      "Iteration 157 - loss: 0.006495873133341472\n",
      "Iteration 158 - loss: 0.006454437971115112\n",
      "Iteration 159 - loss: 0.0064133405685424805\n",
      "Iteration 160 - loss: 0.006372888882954915\n",
      "Iteration 161 - loss: 0.006332914034525554\n",
      "Iteration 162 - loss: 0.006293078263600667\n",
      "Iteration 163 - loss: 0.006254225969314575\n",
      "Iteration 164 - loss: 0.006215691566467285\n",
      "Iteration 165 - loss: 0.006177276372909546\n",
      "Iteration 166 - loss: 0.0061400532722473145\n",
      "Iteration 167 - loss: 0.006102472543716431\n",
      "Iteration 168 - loss: 0.006065845489501953\n",
      "Iteration 169 - loss: 0.006029725074768066\n",
      "Iteration 170 - loss: 0.005994051694869995\n",
      "Iteration 171 - loss: 0.005958378314971924\n",
      "Iteration 172 - loss: 0.005923211574554443\n",
      "Iteration 173 - loss: 0.00588865081469218\n",
      "Iteration 174 - loss: 0.005854626496632894\n",
      "Iteration 175 - loss: 0.005820552508036296\n",
      "Iteration 176 - loss: 0.005787293116251628\n",
      "Iteration 177 - loss: 0.005754053592681885\n",
      "Iteration 178 - loss: 0.005721747875213623\n",
      "Iteration 179 - loss: 0.005689462025960286\n",
      "Iteration 180 - loss: 0.005657166242599487\n",
      "Iteration 181 - loss: 0.005625814199447632\n",
      "Iteration 182 - loss: 0.005594491958618164\n",
      "Iteration 183 - loss: 0.0055636366208394366\n",
      "Iteration 184 - loss: 0.00553294022878011\n",
      "Iteration 185 - loss: 0.005502710739771525\n",
      "Iteration 186 - loss: 0.0054726799329121905\n",
      "Iteration 187 - loss: 0.005443274974822998\n",
      "Iteration 188 - loss: 0.005414009094238281\n",
      "Iteration 189 - loss: 0.005385061105092366\n",
      "Iteration 190 - loss: 0.005356619755427043\n",
      "Iteration 191 - loss: 0.005328158537546794\n",
      "Iteration 192 - loss: 0.005300203959147136\n",
      "Iteration 193 - loss: 0.005272368590037028\n",
      "Iteration 194 - loss: 0.005244910717010498\n",
      "Iteration 195 - loss: 0.005217562119166057\n",
      "Iteration 196 - loss: 0.005190859238306682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 197 - loss: 0.00516401727994283\n",
      "Iteration 198 - loss: 0.005137970050175984\n",
      "Iteration 199 - loss: 0.005111604928970337\n",
      "Iteration 200 - loss: 0.005086004734039307\n",
      "Iteration 201 - loss: 0.005060454209645589\n",
      "Iteration 202 - loss: 0.005034903685251872\n",
      "Iteration 203 - loss: 0.005009939273198445\n",
      "Iteration 204 - loss: 0.00498501459757487\n",
      "Iteration 205 - loss: 0.004960745573043823\n",
      "Iteration 206 - loss: 0.0049362679322560625\n",
      "Iteration 207 - loss: 0.004912147919336955\n",
      "Iteration 208 - loss: 0.004888176918029785\n",
      "Iteration 209 - loss: 0.00486453374226888\n",
      "Iteration 210 - loss: 0.004841357469558716\n",
      "Iteration 211 - loss: 0.004817873239517212\n",
      "Iteration 212 - loss: 0.004795153935750325\n",
      "Iteration 213 - loss: 0.004772156476974487\n",
      "Iteration 214 - loss: 0.004749963680903117\n",
      "Iteration 215 - loss: 0.004727423191070557\n",
      "Iteration 216 - loss: 0.004705369472503662\n",
      "Iteration 217 - loss: 0.004683494567871094\n",
      "Iteration 218 - loss: 0.0046618978182474775\n",
      "Iteration 219 - loss: 0.004640330870946248\n",
      "Iteration 220 - loss: 0.0046189526716868086\n",
      "Iteration 221 - loss: 0.00459785262743632\n",
      "Iteration 222 - loss: 0.0045771002769470215\n",
      "Iteration 223 - loss: 0.004556129376093547\n",
      "Iteration 224 - loss: 0.004535853862762451\n",
      "Iteration 225 - loss: 0.004515250523885091\n",
      "Iteration 226 - loss: 0.00449495514233907\n",
      "Iteration 227 - loss: 0.004475136597951253\n",
      "Iteration 228 - loss: 0.004455496867497762\n",
      "Iteration 229 - loss: 0.004435678323109944\n",
      "Iteration 230 - loss: 0.004416028658548991\n",
      "Iteration 231 - loss: 0.00439685583114624\n",
      "Iteration 232 - loss: 0.004377692937850952\n",
      "Iteration 233 - loss: 0.004358698924382527\n",
      "Iteration 234 - loss: 0.004339993000030518\n",
      "Iteration 235 - loss: 0.004321287075678508\n",
      "Iteration 236 - loss: 0.004302948713302612\n",
      "Iteration 237 - loss: 0.004284590482711792\n",
      "Iteration 238 - loss: 0.00426634152730306\n",
      "Iteration 239 - loss: 0.0042481621106465655\n",
      "Iteration 240 - loss: 0.004230409860610962\n",
      "Iteration 241 - loss: 0.004212538401285808\n",
      "Iteration 242 - loss: 0.004195253054300944\n",
      "Iteration 243 - loss: 0.00417788823445638\n",
      "Iteration 244 - loss: 0.004160443941752116\n",
      "Iteration 245 - loss: 0.004143208265304565\n",
      "Iteration 246 - loss: 0.004126429557800293\n",
      "Iteration 247 - loss: 0.004109491904576619\n",
      "Iteration 248 - loss: 0.004092742999394734\n",
      "Iteration 249 - loss: 0.004075954357783\n",
      "Iteration 250 - loss: 0.004059652487436931\n",
      "Iteration 251 - loss: 0.004043211539586385\n",
      "Iteration 252 - loss: 0.004026919603347778\n",
      "Iteration 253 - loss: 0.0040109654267628985\n",
      "Iteration 254 - loss: 0.00399513045946757\n",
      "Iteration 255 - loss: 0.003979335228602092\n",
      "Iteration 256 - loss: 0.003963539997736613\n",
      "Iteration 257 - loss: 0.003948042790095012\n",
      "Iteration 258 - loss: 0.00393254558245341\n",
      "Iteration 259 - loss: 0.003917217254638672\n",
      "Iteration 260 - loss: 0.003901869058609009\n",
      "Iteration 261 - loss: 0.0038870275020599365\n",
      "Iteration 262 - loss: 0.003872007131576538\n",
      "Iteration 263 - loss: 0.0038571556409200034\n",
      "Iteration 264 - loss: 0.0038423041502634683\n",
      "Iteration 265 - loss: 0.0038277904192606607\n",
      "Iteration 266 - loss: 0.0038134058316548667\n",
      "Iteration 267 - loss: 0.0037988523642222085\n",
      "Iteration 268 - loss: 0.0037845075130462646\n",
      "Iteration 269 - loss: 0.0037701427936553955\n",
      "Iteration 270 - loss: 0.003756244977315267\n",
      "Iteration 271 - loss: 0.0037423372268676758\n",
      "Iteration 272 - loss: 0.0037282705307006836\n",
      "Iteration 273 - loss: 0.00371439258257548\n",
      "Iteration 274 - loss: 0.0037008325258890786\n",
      "Iteration 275 - loss: 0.0036874214808146157\n",
      "Iteration 276 - loss: 0.003674010435740153\n",
      "Iteration 277 - loss: 0.00366059939066569\n",
      "Iteration 278 - loss: 0.0036471784114837646\n",
      "Iteration 279 - loss: 0.003634085257848104\n",
      "Iteration 280 - loss: 0.0036208232243855796\n",
      "Iteration 281 - loss: 0.003607759873072306\n",
      "Iteration 282 - loss: 0.0035947958628336587\n",
      "Iteration 283 - loss: 0.0035821994145711264\n",
      "Iteration 284 - loss: 0.003569255272547404\n",
      "Iteration 285 - loss: 0.003556827704111735\n",
      "Iteration 286 - loss: 0.0035441815853118896\n",
      "Iteration 287 - loss: 0.0035317540168762207\n",
      "Iteration 288 - loss: 0.0035193165143330893\n",
      "Iteration 289 - loss: 0.0035071571667989096\n",
      "Iteration 290 - loss: 0.003494858741760254\n",
      "Iteration 291 - loss: 0.0034827391306559243\n",
      "Iteration 292 - loss: 0.003470758597056071\n",
      "Iteration 293 - loss: 0.0034587979316711426\n",
      "Iteration 294 - loss: 0.0034468273321787515\n",
      "Iteration 295 - loss: 0.0034351646900177\n",
      "Iteration 296 - loss: 0.0034233431021372476\n",
      "Iteration 297 - loss: 0.0034117003281911216\n",
      "Iteration 298 - loss: 0.0034002065658569336\n",
      "Iteration 299 - loss: 0.003388891617457072\n",
      "Iteration 300 - loss: 0.0033775269985198975\n",
      "Iteration 301 - loss: 0.0033660531044006348\n",
      "Iteration 302 - loss: 0.0033548573652903237\n",
      "Iteration 303 - loss: 0.0033438503742218018\n",
      "Iteration 304 - loss: 0.003332356611887614\n",
      "Iteration 305 - loss: 0.003321349620819092\n",
      "Iteration 306 - loss: 0.0033106605211893716\n",
      "Iteration 307 - loss: 0.003299782673517863\n",
      "Iteration 308 - loss: 0.003289093573888143\n",
      "Iteration 309 - loss: 0.003278076648712158\n",
      "Iteration 310 - loss: 0.0032675464948018393\n",
      "Iteration 311 - loss: 0.003257026274998983\n",
      "Iteration 312 - loss: 0.0032463173071543374\n",
      "Iteration 313 - loss: 0.003235777219136556\n",
      "Iteration 314 - loss: 0.00322572390238444\n",
      "Iteration 315 - loss: 0.0032152036825815835\n",
      "Iteration 316 - loss: 0.0032049715518951416\n",
      "Iteration 317 - loss: 0.003194590409596761\n",
      "Iteration 318 - loss: 0.003184547026952108\n",
      "Iteration 319 - loss: 0.0031743446985880532\n",
      "Iteration 320 - loss: 0.003164599339167277\n",
      "Iteration 321 - loss: 0.003154685099919637\n",
      "Iteration 322 - loss: 0.003144641717274984\n",
      "Iteration 323 - loss: 0.0031348963578542075\n",
      "Iteration 324 - loss: 0.003125349680582682\n",
      "Iteration 325 - loss: 0.003115276495615641\n",
      "Iteration 326 - loss: 0.003105719884236654\n",
      "Iteration 327 - loss: 0.003096123536427816\n",
      "Iteration 328 - loss: 0.0030868748823801675\n",
      "Iteration 329 - loss: 0.0030772884686787925\n",
      "Iteration 330 - loss: 0.0030677119890848794\n",
      "Iteration 331 - loss: 0.00305861234664917\n",
      "Iteration 332 - loss: 0.003049383560816447\n",
      "Iteration 333 - loss: 0.003039975961049398\n",
      "Iteration 334 - loss: 0.003030677636464437\n",
      "Iteration 335 - loss: 0.0030216177304585776\n",
      "Iteration 336 - loss: 0.003012498219807943\n",
      "Iteration 337 - loss: 0.0030034085114796958\n",
      "Iteration 338 - loss: 0.002994626760482788\n",
      "Iteration 339 - loss: 0.0029856860637664795\n",
      "Iteration 340 - loss: 0.002976904312769572\n",
      "Iteration 341 - loss: 0.0029683013757069907\n",
      "Iteration 342 - loss: 0.0029592116673787436\n",
      "Iteration 343 - loss: 0.0029505987962086997\n",
      "Iteration 344 - loss: 0.002941956122716268\n",
      "Iteration 345 - loss: 0.002933373053868612\n",
      "Iteration 346 - loss: 0.0029247204462687173\n",
      "Iteration 347 - loss: 0.0029164552688598633\n",
      "Iteration 348 - loss: 0.002907832463582357\n",
      "Iteration 349 - loss: 0.0028996864954630532\n",
      "Iteration 350 - loss: 0.002891063690185547\n",
      "Iteration 351 - loss: 0.0028829475243886313\n",
      "Iteration 352 - loss: 0.0028746426105499268\n",
      "Iteration 353 - loss: 0.0028664767742156982\n",
      "Iteration 354 - loss: 0.0028583407402038574\n",
      "Iteration 355 - loss: 0.002850214640299479\n",
      "Iteration 356 - loss: 0.002842078606287638\n",
      "Iteration 357 - loss: 0.0028339425722757974\n",
      "Iteration 358 - loss: 0.0028261144955952964\n",
      "Iteration 359 - loss: 0.0028181572755177817\n",
      "Iteration 360 - loss: 0.0028103291988372803\n",
      "Iteration 361 - loss: 0.0028021931648254395\n",
      "Iteration 362 - loss: 0.002794543902079264\n",
      "Iteration 363 - loss: 0.0027868847052256265\n",
      "Iteration 364 - loss: 0.0027792155742645264\n",
      "Iteration 365 - loss: 0.002771526575088501\n",
      "Iteration 366 - loss: 0.002763758103052775\n",
      "Iteration 367 - loss: 0.0027560790379842124\n",
      "Iteration 368 - loss: 0.002748568852742513\n",
      "Iteration 369 - loss: 0.00274123748143514\n",
      "Iteration 370 - loss: 0.0027335981527964273\n",
      "Iteration 371 - loss: 0.002726395924886068\n",
      "Iteration 372 - loss: 0.00271873672803243\n",
      "Iteration 373 - loss: 0.0027115444342295327\n",
      "Iteration 374 - loss: 0.0027042130629221597\n",
      "Iteration 375 - loss: 0.002697040637334188\n",
      "Iteration 376 - loss: 0.002689868211746216\n",
      "Iteration 377 - loss: 0.002682675917943319\n",
      "Iteration 378 - loss: 0.002675513426462809\n",
      "Iteration 379 - loss: 0.0026683310667673745\n",
      "Iteration 380 - loss: 0.0026611387729644775\n",
      "Iteration 381 - loss: 0.0026539663473765054\n",
      "Iteration 382 - loss: 0.002647270758946737\n",
      "Iteration 383 - loss: 0.0026400983333587646\n",
      "Iteration 384 - loss: 0.0026332437992095947\n",
      "Iteration 385 - loss: 0.002626518408457438\n",
      "Iteration 386 - loss: 0.0026193459828694663\n",
      "Iteration 387 - loss: 0.0026126702626546225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 388 - loss: 0.002605944871902466\n",
      "Iteration 389 - loss: 0.0025992393493652344\n",
      "Iteration 390 - loss: 0.002592394749323527\n",
      "Iteration 391 - loss: 0.0025856991608937583\n",
      "Iteration 392 - loss: 0.002578993638356527\n",
      "Iteration 393 - loss: 0.0025724669297536216\n",
      "Iteration 394 - loss: 0.002566079298655192\n",
      "Iteration 395 - loss: 0.0025593737761179605\n",
      "Iteration 396 - loss: 0.002552678187688192\n",
      "Iteration 397 - loss: 0.0025464495023091636\n",
      "Iteration 398 - loss: 0.0025397340456644693\n",
      "Iteration 399 - loss: 0.002533515294392904\n",
      "Iteration 400 - loss: 0.002527127663294474\n",
      "Iteration 401 - loss: 0.002520918846130371\n",
      "Iteration 402 - loss: 0.0025143822034200034\n",
      "Iteration 403 - loss: 0.0025079945723215737\n",
      "Iteration 404 - loss: 0.0025017857551574707\n",
      "Iteration 405 - loss: 0.002495567003885905\n",
      "Iteration 406 - loss: 0.0024893383185068765\n",
      "Iteration 407 - loss: 0.0024831295013427734\n",
      "Iteration 408 - loss: 0.0024772187074025473\n",
      "Iteration 409 - loss: 0.00247114896774292\n",
      "Iteration 410 - loss: 0.0024649202823638916\n",
      "Iteration 411 - loss: 0.002459019422531128\n",
      "Iteration 412 - loss: 0.002452979485193888\n",
      "Iteration 413 - loss: 0.0024470587571461997\n",
      "Iteration 414 - loss: 0.0024409989515940347\n",
      "Iteration 415 - loss: 0.0024351179599761963\n",
      "Iteration 416 - loss: 0.002429395914077759\n",
      "Iteration 417 - loss: 0.002423495054244995\n",
      "Iteration 418 - loss: 0.002417405446370443\n",
      "Iteration 419 - loss: 0.0024116734663645425\n",
      "Iteration 420 - loss: 0.0024059414863586426\n",
      "Iteration 421 - loss: 0.0024000306924184165\n",
      "Iteration 422 - loss: 0.0023944278558095298\n",
      "Iteration 423 - loss: 0.0023886958758036294\n",
      "Iteration 424 - loss: 0.0023829638957977295\n",
      "Iteration 425 - loss: 0.0023775299390157065\n",
      "Iteration 426 - loss: 0.002371797959009806\n",
      "Iteration 427 - loss: 0.002366056044896444\n",
      "Iteration 428 - loss: 0.0023607810338338218\n",
      "Iteration 429 - loss: 0.002355039119720459\n",
      "Iteration 430 - loss: 0.002349625031153361\n",
      "Iteration 431 - loss: 0.002344032128651937\n",
      "Iteration 432 - loss: 0.0023386279741923013\n",
      "Iteration 433 - loss: 0.0023330350716908774\n",
      "Iteration 434 - loss: 0.00232777992884318\n",
      "Iteration 435 - loss: 0.0023223459720611572\n",
      "Iteration 436 - loss: 0.0023170908292134604\n",
      "Iteration 437 - loss: 0.0023118158181508384\n",
      "Iteration 438 - loss: 0.0023062328497568765\n",
      "Iteration 439 - loss: 0.0023011465867360434\n",
      "Iteration 440 - loss: 0.002295901377995809\n",
      "Iteration 441 - loss: 0.002290626366933187\n",
      "Iteration 442 - loss: 0.002285351355870565\n",
      "Iteration 443 - loss: 0.0022802352905273438\n",
      "Iteration 444 - loss: 0.0022749602794647217\n",
      "Iteration 445 - loss: 0.0022697349389394126\n",
      "Iteration 446 - loss: 0.0022647778193155923\n",
      "Iteration 447 - loss: 0.0022595226764678955\n",
      "Iteration 448 - loss: 0.0022543867429097495\n",
      "Iteration 449 - loss: 0.002249479293823242\n",
      "Iteration 450 - loss: 0.002244363228480021\n",
      "Iteration 451 - loss: 0.0022394259770711264\n",
      "Iteration 452 - loss: 0.0022344887256622314\n",
      "Iteration 453 - loss: 0.002229382594426473\n",
      "Iteration 454 - loss: 0.0022245744864145913\n",
      "Iteration 455 - loss: 0.002219647169113159\n",
      "Iteration 456 - loss: 0.002214541037877401\n",
      "Iteration 457 - loss: 0.0022097527980804443\n",
      "Iteration 458 - loss: 0.002204964558283488\n",
      "Iteration 459 - loss: 0.0022000173727671304\n",
      "Iteration 460 - loss: 0.002195249001185099\n",
      "Iteration 461 - loss: 0.002190460761388143\n",
      "Iteration 462 - loss: 0.0021856526533762612\n",
      "Iteration 463 - loss: 0.002180894215901693\n",
      "Iteration 464 - loss: 0.0021761258443196616\n",
      "Iteration 465 - loss: 0.0021714866161346436\n",
      "Iteration 466 - loss: 0.0021668573220570884\n",
      "Iteration 467 - loss: 0.0021622379620869956\n",
      "Iteration 468 - loss: 0.0021574397881825766\n",
      "Iteration 469 - loss: 0.0021526614824930825\n",
      "Iteration 470 - loss: 0.0021483798821767173\n",
      "Iteration 471 - loss: 0.0021435817082722983\n",
      "Iteration 472 - loss: 0.0021391212940216064\n",
      "Iteration 473 - loss: 0.0021344820658365884\n",
      "Iteration 474 - loss: 0.0021300017833709717\n",
      "Iteration 475 - loss: 0.0021254122257232666\n",
      "Iteration 476 - loss: 0.00212093194325765\n",
      "Iteration 477 - loss: 0.002116630474726359\n",
      "Iteration 478 - loss: 0.0021118621031443277\n",
      "Iteration 479 - loss: 0.002107540766398112\n",
      "Iteration 480 - loss: 0.002103229363759359\n",
      "Iteration 481 - loss: 0.002098927895228068\n",
      "Iteration 482 - loss: 0.002094477415084839\n",
      "Iteration 483 - loss: 0.0020901858806610107\n",
      "Iteration 484 - loss: 0.0020855466524759927\n",
      "Iteration 485 - loss: 0.0020812352498372397\n",
      "Iteration 486 - loss: 0.0020769337813059487\n",
      "Iteration 487 - loss: 0.002072612444559733\n",
      "Iteration 488 - loss: 0.00206830104192098\n",
      "Iteration 489 - loss: 0.0020643572012583413\n",
      "Iteration 490 - loss: 0.002060025930404663\n",
      "Iteration 491 - loss: 0.0020557045936584473\n",
      "Iteration 492 - loss: 0.0020515620708465576\n",
      "Iteration 493 - loss: 0.0020472705364227295\n",
      "Iteration 494 - loss: 0.002043267091115316\n",
      "Iteration 495 - loss: 0.0020389954249064126\n",
      "Iteration 496 - loss: 0.0020348230997721353\n",
      "Iteration 497 - loss: 0.002030859390894572\n",
      "Iteration 498 - loss: 0.002026547988255819\n",
      "Iteration 499 - loss: 0.002022723356882731\n",
      "Iteration 500 - loss: 0.002018402020136515\n",
      "Iteration 501 - loss: 0.0020144482453664145\n",
      "Iteration 502 - loss: 0.002010275920232137\n",
      "Iteration 503 - loss: 0.002006461222966512\n",
      "Iteration 504 - loss: 0.002002467711766561\n",
      "Iteration 505 - loss: 0.0019983251889546714\n",
      "Iteration 506 - loss: 0.0019944806893666587\n",
      "Iteration 507 - loss: 0.001990516980489095\n",
      "Iteration 508 - loss: 0.0019863744576772055\n",
      "Iteration 509 - loss: 0.0019825597604115805\n",
      "Iteration 510 - loss: 0.0019785662492116294\n",
      "Iteration 511 - loss: 0.0019747416178385415\n",
      "Iteration 512 - loss: 0.001970897118250529\n",
      "Iteration 513 - loss: 0.0019670724868774414\n",
      "Iteration 514 - loss: 0.001963267723719279\n",
      "Iteration 515 - loss: 0.0019594033559163413\n",
      "Iteration 516 - loss: 0.001955618460973104\n",
      "Iteration 517 - loss: 0.001951773961385091\n",
      "Iteration 518 - loss: 0.001947939395904541\n",
      "Iteration 519 - loss: 0.001944124698638916\n",
      "Iteration 520 - loss: 0.0019403199354807537\n",
      "Iteration 521 - loss: 0.0019364555676778157\n",
      "Iteration 522 - loss: 0.0019329289595286052\n",
      "Iteration 523 - loss: 0.0019293030103047688\n",
      "Iteration 524 - loss: 0.0019254585107167561\n",
      "Iteration 525 - loss: 0.0019219716389973958\n",
      "Iteration 526 - loss: 0.0019181172053019206\n",
      "Iteration 527 - loss: 0.0019144614537556965\n",
      "Iteration 528 - loss: 0.0019106368223826091\n",
      "Iteration 529 - loss: 0.0019072691599527996\n",
      "Iteration 530 - loss: 0.0019034743309020996\n",
      "Iteration 531 - loss: 0.001899947722752889\n",
      "Iteration 532 - loss: 0.0018963019053141277\n",
      "Iteration 533 - loss: 0.001892795165379842\n",
      "Iteration 534 - loss: 0.0018891294797261555\n",
      "Iteration 535 - loss: 0.0018856227397918701\n",
      "Iteration 536 - loss: 0.0018822650114695232\n",
      "Iteration 537 - loss: 0.001878430445988973\n",
      "Iteration 538 - loss: 0.001875092585881551\n",
      "Iteration 539 - loss: 0.0018715858459472656\n",
      "Iteration 540 - loss: 0.0018679102261861165\n",
      "Iteration 541 - loss: 0.0018645524978637695\n",
      "Iteration 542 - loss: 0.0018610854943593342\n",
      "Iteration 543 - loss: 0.0018577178319295247\n",
      "Iteration 544 - loss: 0.001854350169499715\n",
      "Iteration 545 - loss: 0.0018507142861684163\n",
      "Iteration 546 - loss: 0.001847366491953532\n",
      "Iteration 547 - loss: 0.0018439888954162598\n",
      "Iteration 548 - loss: 0.0018406510353088379\n",
      "Iteration 549 - loss: 0.001837293306986491\n",
      "Iteration 550 - loss: 0.0018338263034820557\n",
      "Iteration 551 - loss: 0.0018304487069447835\n",
      "Iteration 552 - loss: 0.0018271207809448242\n",
      "Iteration 553 - loss: 0.0018237531185150146\n",
      "Iteration 554 - loss: 0.0018205642700195312\n",
      "Iteration 555 - loss: 0.0018172164758046467\n",
      "Iteration 556 - loss: 0.001814196507136027\n",
      "Iteration 557 - loss: 0.00181083877881368\n",
      "Iteration 558 - loss: 0.001807500918706258\n",
      "Iteration 559 - loss: 0.0018041531244913738\n",
      "Iteration 560 - loss: 0.001800815264383952\n",
      "Iteration 561 - loss: 0.0017977853616078694\n",
      "Iteration 562 - loss: 0.001794576644897461\n",
      "Iteration 563 - loss: 0.0017912288506825764\n",
      "Iteration 564 - loss: 0.0017878909905751546\n",
      "Iteration 565 - loss: 0.0017850399017333984\n",
      "Iteration 566 - loss: 0.0017816722393035889\n",
      "Iteration 567 - loss: 0.0017786721388498943\n",
      "Iteration 568 - loss: 0.0017754534880320232\n",
      "Iteration 569 - loss: 0.0017721156279246013\n",
      "Iteration 570 - loss: 0.0017690956592559814\n",
      "Iteration 571 - loss: 0.001765906810760498\n",
      "Iteration 572 - loss: 0.0017628669738769531\n",
      "Iteration 573 - loss: 0.0017596781253814697\n",
      "Iteration 574 - loss: 0.0017566680908203125\n",
      "Iteration 575 - loss: 0.001753777265548706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 576 - loss: 0.0017505983511606853\n",
      "Iteration 577 - loss: 0.001747588316599528\n",
      "Iteration 578 - loss: 0.001744687557220459\n",
      "Iteration 579 - loss: 0.001741488774617513\n",
      "Iteration 580 - loss: 0.0017384688059488933\n",
      "Iteration 581 - loss: 0.0017356276512145996\n",
      "Iteration 582 - loss: 0.0017324189345041912\n",
      "Iteration 583 - loss: 0.0017293890317281087\n",
      "Iteration 584 - loss: 0.00172652800877889\n",
      "Iteration 585 - loss: 0.0017236669858296712\n",
      "Iteration 586 - loss: 0.0017204682032267253\n",
      "Iteration 587 - loss: 0.001717597246170044\n",
      "Iteration 588 - loss: 0.001714557409286499\n",
      "Iteration 589 - loss: 0.0017117361227671306\n",
      "Iteration 590 - loss: 0.0017088552316029866\n",
      "Iteration 591 - loss: 0.0017059644063313801\n",
      "Iteration 592 - loss: 0.001703113317489624\n",
      "Iteration 593 - loss: 0.0017002324263254802\n",
      "Iteration 594 - loss: 0.0016972124576568604\n",
      "Iteration 595 - loss: 0.00169450044631958\n",
      "Iteration 596 - loss: 0.0016914904117584229\n",
      "Iteration 597 - loss: 0.0016886194547017415\n",
      "Iteration 598 - loss: 0.0016858975092569988\n",
      "Iteration 599 - loss: 0.0016830166180928547\n",
      "Iteration 600 - loss: 0.0016801456610361736\n",
      "Iteration 601 - loss: 0.0016772945721944172\n",
      "Iteration 602 - loss: 0.001674721638361613\n",
      "Iteration 603 - loss: 0.0016718804836273193\n",
      "Iteration 604 - loss: 0.001669009526570638\n",
      "Iteration 605 - loss: 0.001666108767191569\n",
      "Iteration 606 - loss: 0.0016633967558542888\n",
      "Iteration 607 - loss: 0.001660853624343872\n",
      "Iteration 608 - loss: 0.0016580124696095784\n",
      "Iteration 609 - loss: 0.0016551315784454346\n",
      "Iteration 610 - loss: 0.0016523996988932292\n",
      "Iteration 611 - loss: 0.0016498863697052002\n",
      "Iteration 612 - loss: 0.0016469955444335938\n",
      "Iteration 613 - loss: 0.001644134521484375\n",
      "Iteration 614 - loss: 0.001641760269800822\n",
      "Iteration 615 - loss: 0.0016388893127441406\n",
      "Iteration 616 - loss: 0.0016364951928456624\n",
      "Iteration 617 - loss: 0.0016336242357889812\n",
      "Iteration 618 - loss: 0.0016310811042785645\n",
      "Iteration 619 - loss: 0.0016283591588338215\n",
      "Iteration 620 - loss: 0.001625468333562215\n",
      "Iteration 621 - loss: 0.0016230940818786621\n",
      "Iteration 622 - loss: 0.001620541016260783\n",
      "Iteration 623 - loss: 0.001617838939030965\n",
      "Iteration 624 - loss: 0.0016152958075205486\n",
      "Iteration 625 - loss: 0.0016125837961832683\n",
      "Iteration 626 - loss: 0.0016100108623504639\n",
      "Iteration 627 - loss: 0.001607338587443034\n",
      "Iteration 628 - loss: 0.0016047755877176921\n",
      "Iteration 629 - loss: 0.0016023715337117512\n",
      "Iteration 630 - loss: 0.0015996694564819336\n",
      "Iteration 631 - loss: 0.0015971362590789795\n",
      "Iteration 632 - loss: 0.0015947222709655762\n",
      "Iteration 633 - loss: 0.001592030127843221\n",
      "Iteration 634 - loss: 0.0015896260738372803\n",
      "Iteration 635 - loss: 0.0015870829423268635\n",
      "Iteration 636 - loss: 0.0015846888224283855\n",
      "Iteration 637 - loss: 0.0015819867451985676\n",
      "Iteration 638 - loss: 0.0015794436136881511\n",
      "Iteration 639 - loss: 0.0015770494937896729\n",
      "Iteration 640 - loss: 0.0015746454397837322\n",
      "Iteration 641 - loss: 0.0015722413857777913\n",
      "Iteration 642 - loss: 0.0015698671340942383\n",
      "Iteration 643 - loss: 0.001566996177037557\n",
      "Iteration 644 - loss: 0.001564621925354004\n",
      "Iteration 645 - loss: 0.0015622079372406006\n",
      "Iteration 646 - loss: 0.001559823751449585\n",
      "Iteration 647 - loss: 0.001557449499766032\n",
      "Iteration 648 - loss: 0.0015550553798675537\n",
      "Iteration 649 - loss: 0.001552671194076538\n",
      "Iteration 650 - loss: 0.0015502671400705974\n",
      "Iteration 651 - loss: 0.0015478730201721191\n",
      "Iteration 652 - loss: 0.0015454888343811035\n",
      "Iteration 653 - loss: 0.0015430847803751628\n",
      "Iteration 654 - loss: 0.0015407105286916096\n",
      "Iteration 655 - loss: 0.0015382965405782063\n",
      "Iteration 656 - loss: 0.0015359421571095784\n",
      "Iteration 657 - loss: 0.001533528168996175\n",
      "Iteration 658 - loss: 0.0015311439832051594\n",
      "Iteration 659 - loss: 0.0015287498633066814\n",
      "Iteration 660 - loss: 0.0015266835689544678\n",
      "Iteration 661 - loss: 0.0015243093172709148\n",
      "Iteration 662 - loss: 0.0015219151973724365\n",
      "Iteration 663 - loss: 0.0015196800231933594\n",
      "Iteration 664 - loss: 0.0015172759691874187\n",
      "Iteration 665 - loss: 0.00151518980662028\n",
      "Iteration 666 - loss: 0.001512835423151652\n",
      "Iteration 667 - loss: 0.0015104313691457112\n",
      "Iteration 668 - loss: 0.001508176326751709\n",
      "Iteration 669 - loss: 0.001505802075068156\n",
      "Iteration 670 - loss: 0.0015037357807159424\n",
      "Iteration 671 - loss: 0.0015014906724294026\n",
      "Iteration 672 - loss: 0.001499106486638387\n",
      "Iteration 673 - loss: 0.0014970401922861736\n",
      "Iteration 674 - loss: 0.0014946460723876953\n",
      "Iteration 675 - loss: 0.0014924108982086182\n",
      "Iteration 676 - loss: 0.0014903545379638672\n",
      "Iteration 677 - loss: 0.0014879504839579265\n",
      "Iteration 678 - loss: 0.0014857053756713867\n",
      "Iteration 679 - loss: 0.0014836589495340984\n",
      "Iteration 680 - loss: 0.0014814138412475586\n",
      "Iteration 681 - loss: 0.0014793674151102703\n",
      "Iteration 682 - loss: 0.0014771223068237305\n",
      "Iteration 683 - loss: 0.0014750560124715169\n",
      "Iteration 684 - loss: 0.0014726420243581135\n",
      "Iteration 685 - loss: 0.001470734675725301\n",
      "Iteration 686 - loss: 0.001468360424041748\n",
      "Iteration 687 - loss: 0.0014664530754089355\n",
      "Iteration 688 - loss: 0.0014640589555104573\n",
      "Iteration 689 - loss: 0.0014621416727701824\n",
      "Iteration 690 - loss: 0.001459747552871704\n",
      "Iteration 691 - loss: 0.0014578402042388916\n",
      "Iteration 692 - loss: 0.0014554460843404133\n",
      "Iteration 693 - loss: 0.0014535486698150635\n",
      "Iteration 694 - loss: 0.001451621452967326\n",
      "Iteration 695 - loss: 0.0014492472012837727\n",
      "Iteration 696 - loss: 0.0014473199844360352\n",
      "Iteration 697 - loss: 0.0014452735582987468\n",
      "Iteration 698 - loss: 0.0014430185159047444\n",
      "Iteration 699 - loss: 0.001441111167271932\n",
      "Iteration 700 - loss: 0.0014390448729197185\n",
      "Iteration 701 - loss: 0.0014368196328481038\n",
      "Iteration 702 - loss: 0.0014347632726033528\n",
      "Iteration 703 - loss: 0.0014328459898630779\n",
      "Iteration 704 - loss: 0.0014306108156840007\n",
      "Iteration 705 - loss: 0.0014286935329437256\n",
      "Iteration 706 - loss: 0.0014266173044840496\n",
      "Iteration 707 - loss: 0.0014246900876363118\n",
      "Iteration 708 - loss: 0.0014224648475646973\n",
      "Iteration 709 - loss: 0.0014205674330393474\n",
      "Iteration 710 - loss: 0.0014184812704722087\n",
      "Iteration 711 - loss: 0.0014165838559468587\n",
      "Iteration 712 - loss: 0.0014146467049916585\n",
      "Iteration 713 - loss: 0.0014124313990275066\n",
      "Iteration 714 - loss: 0.001410384972890218\n",
      "Iteration 715 - loss: 0.0014085968335469563\n",
      "Iteration 716 - loss: 0.001406530539194743\n",
      "Iteration 717 - loss: 0.0014046231905619304\n",
      "Iteration 718 - loss: 0.0014027059078216553\n",
      "Iteration 719 - loss: 0.0014007886250813801\n",
      "Iteration 720 - loss: 0.0013988912105560303\n",
      "Iteration 721 - loss: 0.0013969739278157551\n",
      "Iteration 722 - loss: 0.001394738753636678\n",
      "Iteration 723 - loss: 0.0013926923274993896\n",
      "Iteration 724 - loss: 0.001390924056371053\n",
      "Iteration 725 - loss: 0.0013889968395233154\n",
      "Iteration 726 - loss: 0.0013869603474934895\n",
      "Iteration 727 - loss: 0.0013850231965382893\n",
      "Iteration 728 - loss: 0.0013831257820129395\n",
      "Iteration 729 - loss: 0.0013813475767771404\n",
      "Iteration 730 - loss: 0.0013794501622517903\n",
      "Iteration 731 - loss: 0.0013775428136189778\n",
      "Iteration 732 - loss: 0.0013754963874816895\n",
      "Iteration 733 - loss: 0.0013737082481384277\n",
      "Iteration 734 - loss: 0.0013718008995056152\n",
      "Iteration 735 - loss: 0.0013702015082041423\n",
      "Iteration 736 - loss: 0.0013682842254638672\n",
      "Iteration 737 - loss: 0.001366366942723592\n",
      "Iteration 738 - loss: 0.0013644893964131672\n",
      "Iteration 739 - loss: 0.0013625820477803547\n",
      "Iteration 740 - loss: 0.0013606647650400798\n",
      "Iteration 741 - loss: 0.001358737548192342\n",
      "Iteration 742 - loss: 0.0013571679592132568\n",
      "Iteration 743 - loss: 0.0013552308082580566\n",
      "Iteration 744 - loss: 0.00135346253712972\n",
      "Iteration 745 - loss: 0.0013515651226043701\n",
      "Iteration 746 - loss: 0.0013496875762939453\n",
      "Iteration 747 - loss: 0.0013477504253387451\n",
      "Iteration 748 - loss: 0.0013461609681447346\n",
      "Iteration 749 - loss: 0.0013442536195119221\n",
      "Iteration 750 - loss: 0.001342475414276123\n",
      "Iteration 751 - loss: 0.0013405680656433105\n",
      "Iteration 752 - loss: 0.0013389686743418376\n",
      "Iteration 753 - loss: 0.0013370811939239502\n",
      "Iteration 754 - loss: 0.0013353228569030762\n",
      "Iteration 755 - loss: 0.001333405574162801\n",
      "Iteration 756 - loss: 0.0013318061828613281\n",
      "Iteration 757 - loss: 0.001330047845840454\n",
      "Iteration 758 - loss: 0.0013281603654225667\n",
      "Iteration 759 - loss: 0.0013265510400136311\n",
      "Iteration 760 - loss: 0.0013248026371002197\n",
      "Iteration 761 - loss: 0.0013229151566823323\n",
      "Iteration 762 - loss: 0.0013213058312733967\n",
      "Iteration 763 - loss: 0.0013194084167480469\n",
      "Iteration 764 - loss: 0.0013176302115122478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 765 - loss: 0.0013160506884257\n",
      "Iteration 766 - loss: 0.0013142724831899006\n",
      "Iteration 767 - loss: 0.0013127028942108154\n",
      "Iteration 768 - loss: 0.0013108054796854656\n",
      "Iteration 769 - loss: 0.001309057076772054\n",
      "Iteration 770 - loss: 0.0013074278831481934\n",
      "Iteration 771 - loss: 0.001305679480234782\n",
      "Iteration 772 - loss: 0.0013040900230407715\n",
      "Iteration 773 - loss: 0.0013023316860198975\n",
      "Iteration 774 - loss: 0.0013007521629333496\n",
      "Iteration 775 - loss: 0.0012988348801930745\n",
      "Iteration 776 - loss: 0.0012973944346110027\n",
      "Iteration 777 - loss: 0.0012955069541931152\n",
      "Iteration 778 - loss: 0.0012940665086110432\n",
      "Iteration 779 - loss: 0.0012921392917633057\n",
      "Iteration 780 - loss: 0.0012906889120737712\n",
      "Iteration 781 - loss: 0.0012888014316558838\n",
      "Iteration 782 - loss: 0.0012873709201812744\n",
      "Iteration 783 - loss: 0.0012854735056559246\n",
      "Iteration 784 - loss: 0.00128402312596639\n",
      "Iteration 785 - loss: 0.001282443602879842\n",
      "Iteration 786 - loss: 0.001280665397644043\n",
      "Iteration 787 - loss: 0.0012792348861694336\n",
      "Iteration 788 - loss: 0.0012773374716440837\n",
      "Iteration 789 - loss: 0.0012759069601694744\n",
      "Iteration 790 - loss: 0.0012739797433217366\n",
      "Iteration 791 - loss: 0.0012725492318471272\n",
      "Iteration 792 - loss: 0.0012711187203725178\n",
      "Iteration 793 - loss: 0.0012692113717397053\n",
      "Iteration 794 - loss: 0.0012677709261576335\n",
      "Iteration 795 - loss: 0.0012663404146830242\n",
      "Iteration 796 - loss: 0.001264423131942749\n",
      "Iteration 797 - loss: 0.001262982686360677\n",
      "Iteration 798 - loss: 0.0012615521748860676\n",
      "Iteration 799 - loss: 0.0012596746285756428\n",
      "Iteration 800 - loss: 0.0012582242488861084\n",
      "Iteration 801 - loss: 0.0012567838033040364\n",
      "Iteration 802 - loss: 0.0012548863887786865\n",
      "Iteration 803 - loss: 0.001253436009089152\n",
      "Iteration 804 - loss: 0.0012519856293996174\n",
      "Iteration 805 - loss: 0.0012504160404205322\n",
      "Iteration 806 - loss: 0.0012486378351847331\n",
      "Iteration 807 - loss: 0.0012472371260325115\n",
      "Iteration 808 - loss: 0.0012456576029459636\n",
      "Iteration 809 - loss: 0.0012441972891489665\n",
      "Iteration 810 - loss: 0.001242448886235555\n",
      "Iteration 811 - loss: 0.0012410283088684082\n",
      "Iteration 812 - loss: 0.0012395679950714111\n",
      "Iteration 813 - loss: 0.0012379785378774006\n",
      "Iteration 814 - loss: 0.0012365480264027913\n",
      "Iteration 815 - loss: 0.0012347896893819172\n",
      "Iteration 816 - loss: 0.0012333790461222331\n",
      "Iteration 817 - loss: 0.00123177965482076\n",
      "Iteration 818 - loss: 0.0012305080890655518\n",
      "Iteration 819 - loss: 0.0012289086977640788\n",
      "Iteration 820 - loss: 0.0012274682521820068\n",
      "Iteration 821 - loss: 0.0012257099151611328\n",
      "Iteration 822 - loss: 0.0012242694695790608\n",
      "Iteration 823 - loss: 0.0012228588263193767\n",
      "Iteration 824 - loss: 0.001221408446629842\n",
      "Iteration 825 - loss: 0.0012198189894358318\n",
      "Iteration 826 - loss: 0.0012184083461761475\n",
      "Iteration 827 - loss: 0.001216957966486613\n",
      "Iteration 828 - loss: 0.0012155473232269287\n",
      "Iteration 829 - loss: 0.0012140870094299316\n",
      "Iteration 830 - loss: 0.0012126763661702473\n",
      "Iteration 831 - loss: 0.0012108981609344482\n",
      "Iteration 832 - loss: 0.0012094775835673015\n",
      "Iteration 833 - loss: 0.0012080172697703044\n",
      "Iteration 834 - loss: 0.0012066165606180828\n",
      "Iteration 835 - loss: 0.0012050271034240723\n",
      "Iteration 836 - loss: 0.0012037257353464763\n",
      "Iteration 837 - loss: 0.001202295223871867\n",
      "Iteration 838 - loss: 0.0012007057666778564\n",
      "Iteration 839 - loss: 0.0011994342009226482\n",
      "Iteration 840 - loss: 0.0011980036894480388\n",
      "Iteration 841 - loss: 0.0011964142322540283\n",
      "Iteration 842 - loss: 0.0011951327323913574\n",
      "Iteration 843 - loss: 0.001193543275197347\n",
      "Iteration 844 - loss: 0.0011922816435496013\n",
      "Iteration 845 - loss: 0.0011906822522481282\n",
      "Iteration 846 - loss: 0.0011894007523854573\n",
      "Iteration 847 - loss: 0.0011879603068033855\n",
      "Iteration 848 - loss: 0.0011865496635437012\n",
      "Iteration 849 - loss: 0.0011854271094004314\n",
      "Iteration 850 - loss: 0.0011839767297108967\n",
      "Iteration 851 - loss: 0.0011825660864512126\n",
      "Iteration 852 - loss: 0.001181115706761678\n",
      "Iteration 853 - loss: 0.0011796653270721436\n",
      "Iteration 854 - loss: 0.0011782745520273845\n",
      "Iteration 855 - loss: 0.0011768241723378499\n",
      "Iteration 856 - loss: 0.0011754035949707031\n",
      "Iteration 857 - loss: 0.001173943281173706\n",
      "Iteration 858 - loss: 0.0011725227038065593\n",
      "Iteration 859 - loss: 0.0011715491612752278\n",
      "Iteration 860 - loss: 0.0011701186498006184\n",
      "Iteration 861 - loss: 0.0011686980724334717\n",
      "Iteration 862 - loss: 0.001167277495066325\n",
      "Iteration 863 - loss: 0.0011658072471618652\n",
      "Iteration 864 - loss: 0.001164396603902181\n",
      "Iteration 865 - loss: 0.0011634528636932373\n",
      "Iteration 866 - loss: 0.001162022352218628\n",
      "Iteration 867 - loss: 0.0011605918407440186\n",
      "Iteration 868 - loss: 0.0011591513951619465\n",
      "Iteration 869 - loss: 0.0011577208836873372\n",
      "Iteration 870 - loss: 0.0011565983295440674\n",
      "Iteration 871 - loss: 0.0011553267637888591\n",
      "Iteration 872 - loss: 0.0011538863182067871\n",
      "Iteration 873 - loss: 0.001152445872624715\n",
      "Iteration 874 - loss: 0.0011510054270426433\n",
      "Iteration 875 - loss: 0.0011500616868336995\n",
      "Iteration 876 - loss: 0.0011486212412516277\n",
      "Iteration 877 - loss: 0.0011472105979919434\n",
      "Iteration 878 - loss: 0.0011457701524098713\n",
      "Iteration 879 - loss: 0.00114479660987854\n",
      "Iteration 880 - loss: 0.0011433859666188557\n",
      "Iteration 881 - loss: 0.0011419355869293213\n",
      "Iteration 882 - loss: 0.0011404852072397869\n",
      "Iteration 883 - loss: 0.0011393924554189046\n",
      "Iteration 884 - loss: 0.0011380910873413086\n",
      "Iteration 885 - loss: 0.0011366804440816243\n",
      "Iteration 886 - loss: 0.0011355578899383545\n",
      "Iteration 887 - loss: 0.0011342962582906087\n",
      "Iteration 888 - loss: 0.001132875680923462\n",
      "Iteration 889 - loss: 0.0011317332585652669\n",
      "Iteration 890 - loss: 0.0011304716269175212\n",
      "Iteration 891 - loss: 0.0011290510495503743\n",
      "Iteration 892 - loss: 0.0011279086271921794\n",
      "Iteration 893 - loss: 0.001126637061436971\n",
      "Iteration 894 - loss: 0.0011252065499623616\n",
      "Iteration 895 - loss: 0.0011241038640340169\n",
      "Iteration 896 - loss: 0.0011228322982788086\n",
      "Iteration 897 - loss: 0.0011214017868041992\n",
      "Iteration 898 - loss: 0.0011202991008758545\n",
      "Iteration 899 - loss: 0.0011190275351206462\n",
      "Iteration 900 - loss: 0.0011179049809773762\n",
      "Iteration 901 - loss: 0.0011164744695027669\n",
      "Iteration 902 - loss: 0.001115192969640096\n",
      "Iteration 903 - loss: 0.0011140803496042888\n",
      "Iteration 904 - loss: 0.001112798849741618\n",
      "Iteration 905 - loss: 0.001111676295598348\n",
      "Iteration 906 - loss: 0.001110394795735677\n",
      "Iteration 907 - loss: 0.0011089543501536052\n",
      "Iteration 908 - loss: 0.001107861598332723\n",
      "Iteration 909 - loss: 0.0011065701643625896\n",
      "Iteration 910 - loss: 0.0011054674784342449\n",
      "Iteration 911 - loss: 0.0011042157808939617\n",
      "Iteration 912 - loss: 0.0011030832926432292\n",
      "Iteration 913 - loss: 0.0011017918586730957\n",
      "Iteration 914 - loss: 0.0011006991068522136\n",
      "Iteration 915 - loss: 0.00109940767288208\n",
      "Iteration 916 - loss: 0.0010983049869537354\n",
      "Iteration 917 - loss: 0.0010970135529836018\n",
      "Iteration 918 - loss: 0.001095910867055257\n",
      "Iteration 919 - loss: 0.0010946194330851238\n",
      "Iteration 920 - loss: 0.001093516747156779\n",
      "Iteration 921 - loss: 0.001092096169789632\n",
      "Iteration 922 - loss: 0.0010911126931508381\n",
      "Iteration 923 - loss: 0.0010897119839986165\n",
      "Iteration 924 - loss: 0.0010887583096822102\n",
      "Iteration 925 - loss: 0.0010872979958852131\n",
      "Iteration 926 - loss: 0.0010863542556762695\n",
      "Iteration 927 - loss: 0.0010849237442016602\n",
      "Iteration 928 - loss: 0.001083970069885254\n",
      "Iteration 929 - loss: 0.0010825395584106445\n",
      "Iteration 930 - loss: 0.0010815858840942383\n",
      "Iteration 931 - loss: 0.00108031431833903\n",
      "Iteration 932 - loss: 0.0010791619618733723\n",
      "Iteration 933 - loss: 0.001078208287556966\n",
      "Iteration 934 - loss: 0.0010769367218017578\n",
      "Iteration 935 - loss: 0.001075814167658488\n",
      "Iteration 936 - loss: 0.0010744333267211914\n",
      "Iteration 937 - loss: 0.0010734697182973225\n",
      "Iteration 938 - loss: 0.0010725160439809163\n",
      "Iteration 939 - loss: 0.0010710755983988445\n",
      "Iteration 940 - loss: 0.0010701119899749756\n",
      "Iteration 941 - loss: 0.0010686715443929036\n",
      "Iteration 942 - loss: 0.001067707935969035\n",
      "Iteration 943 - loss: 0.0010667741298675537\n",
      "Iteration 944 - loss: 0.0010653336842854817\n",
      "Iteration 945 - loss: 0.0010643601417541504\n",
      "Iteration 946 - loss: 0.0010632673899332683\n",
      "Iteration 947 - loss: 0.0010619759559631348\n",
      "Iteration 948 - loss: 0.0010610123475392659\n",
      "Iteration 949 - loss: 0.0010595917701721191\n",
      "Iteration 950 - loss: 0.0010586380958557129\n",
      "Iteration 951 - loss: 0.0010577042897542317\n",
      "Iteration 952 - loss: 0.0010562539100646973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 953 - loss: 0.0010553101698557537\n",
      "Iteration 954 - loss: 0.0010543366273244221\n",
      "Iteration 955 - loss: 0.0010529160499572754\n",
      "Iteration 956 - loss: 0.001051942507425944\n",
      "Iteration 957 - loss: 0.0010509888331095378\n",
      "Iteration 958 - loss: 0.001049886147181193\n",
      "Iteration 959 - loss: 0.0010485947132110596\n",
      "Iteration 960 - loss: 0.0010476410388946533\n",
      "Iteration 961 - loss: 0.0010466972986857097\n",
      "Iteration 962 - loss: 0.0010452767213185628\n",
      "Iteration 963 - loss: 0.0010442833105723064\n",
      "Iteration 964 - loss: 0.0010433594385782878\n",
      "Iteration 965 - loss: 0.0010422468185424805\n",
      "Iteration 966 - loss: 0.0010409752527872722\n",
      "Iteration 967 - loss: 0.001040021578470866\n",
      "Iteration 968 - loss: 0.0010389089584350586\n",
      "Iteration 969 - loss: 0.0010379552841186523\n",
      "Iteration 970 - loss: 0.001036683718363444\n",
      "Iteration 971 - loss: 0.0010356903076171875\n",
      "Iteration 972 - loss: 0.0010347366333007812\n",
      "Iteration 973 - loss: 0.0010336538155873616\n",
      "Iteration 974 - loss: 0.0010323723157246907\n",
      "Iteration 975 - loss: 0.0010314186414082844\n",
      "Iteration 976 - loss: 0.0010304550329844158\n",
      "Iteration 977 - loss: 0.0010294914245605469\n",
      "Iteration 978 - loss: 0.0010280509789784749\n",
      "Iteration 979 - loss: 0.0010271072387695312\n",
      "Iteration 980 - loss: 0.0010261436303456624\n",
      "Iteration 981 - loss: 0.0010252197583516438\n",
      "Iteration 982 - loss: 0.0010240872701009114\n",
      "Iteration 983 - loss: 0.0010228057702382405\n",
      "Iteration 984 - loss: 0.0010218421618143718\n",
      "Iteration 985 - loss: 0.0010209083557128906\n",
      "Iteration 986 - loss: 0.0010199348131815593\n",
      "Iteration 987 - loss: 0.001018842061360677\n",
      "Iteration 988 - loss: 0.0010178685188293457\n",
      "Iteration 989 - loss: 0.0010165870189666748\n",
      "Iteration 990 - loss: 0.0010156432787577312\n",
      "Iteration 991 - loss: 0.0010146697362263997\n",
      "Iteration 992 - loss: 0.0010137359301249187\n",
      "Iteration 993 - loss: 0.0010127623875935872\n",
      "Iteration 994 - loss: 0.0010116597016652424\n",
      "Iteration 995 - loss: 0.0010107060273488362\n",
      "Iteration 996 - loss: 0.0010094443957010906\n",
      "Iteration 997 - loss: 0.0010085006554921467\n",
      "Iteration 998 - loss: 0.0010075171788533528\n",
      "Iteration 999 - loss: 0.001006414492925008\n",
      "Iteration 1000 - loss: 0.0010055998961130779\n"
     ]
    }
   ],
   "source": [
    "val optimized_weights = training_with_loss(g, training_inputs, training_outputs, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Deeplearning4J implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[         0,         0,    1.0000], \n",
       " [    1.0000,    1.0000,    1.0000], \n",
       " [    1.0000,         0,    1.0000], \n",
       " [         0,    1.0000,    1.0000]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0], \n",
       " [1.0000], \n",
       " [1.0000], \n",
       " [0]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "val ds = DataSet(training_inputs, training_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.nd4j.linalg.learning.config.Sgd\n",
    "import org.nd4j.linalg.activations.Activation\n",
    "import org.nd4j.linalg.learning.config.Nesterovs\n",
    "import org.nd4j.linalg.lossfunctions.LossFunctions\n",
    "\n",
    "\n",
    "val seed = 1234 // number used to initialize a pseudorandom number generator.\n",
    "val nEpochs = 10000 // number of training epochs\n",
    "val numHiddenNodes = 3\n",
    "\n",
    "\n",
    "// https://medium.com/konvergen/momentum-method-and-nesterov-accelerated-gradient-487ba776c987\n",
    "val conf = NeuralNetConfiguration.Builder()\n",
    "        .weightInit(WeightInit.XAVIER)\n",
    "        //.updater(Nesterovs(0.01, 0.9))\n",
    "        .updater(Sgd(0.9))\n",
    "        .seed(seed.toLong())\n",
    "        .list()\n",
    "        .layer(DenseLayer.Builder()\n",
    "                .nIn(3)\n",
    "                .nOut(numHiddenNodes)\n",
    "                .activation(Activation.SIGMOID) // random initialize weights with values between 0 and 1\n",
    "                .build())\n",
    "        .layer(OutputLayer.Builder(LossFunctions.LossFunction.MSE) \n",
    "                .nIn(numHiddenNodes).nOut(1)\n",
    "                .activation(Activation.IDENTITY)\n",
    "                .build())\n",
    "        .build()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=================================================================\n",
      "LayerName (LayerType)   nIn,nOut   TotalParams   ParamsShape     \n",
      "=================================================================\n",
      "layer0 (DenseLayer)     3,3        12            W:{3,3}, b:{1,3}\n",
      "layer1 (OutputLayer)    3,1        4             W:{3,1}, b:{1,1}\n",
      "-----------------------------------------------------------------\n",
      "            Total Parameters:  16\n",
      "        Trainable Parameters:  16\n",
      "           Frozen Parameters:  0\n",
      "=================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.deeplearning4j.nn.api.OptimizationAlgorithm;\n",
    "\n",
    "val net = MultiLayerNetwork(conf)\n",
    "net.init()\n",
    "\n",
    "// add an listener which outputs the error every 100 parameter updates\n",
    "net.setListeners(ScoreIterationListener(100))\n",
    "\n",
    "// C&P from LSTMCharModellingExample\n",
    "// Print the number of parameters in the network (and for each layer)\n",
    "println(net.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "// here the actual learning takes place\n",
    "for (i in 0 until nEpochs) {\n",
    "    net.fit(ds)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun compute_dl4j_neural_network(x1: Number, x2: Number, x3: Number) {\n",
    "    val inputs = Nd4j.create(arrayOf(floatArrayOf(x1.toFloat(), x2.toFloat(), x3.toFloat())))\n",
    "    val output = net.output(inputs)\n",
    "    println(\"input: \"+ x1 +\", \"+ x2 +\", \"+ x3 +\" | dl4j output = \"+ output)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 0, 0, 1 | output = [[0.0002]]\n",
      "input: 1, 1, 1 | output = [[0.9999]]\n",
      "input: 1, 0, 1 | output = [[0.9999]]\n",
      "input: 0, 1, 1 | output = [[0.0001]]\n",
      "Prediction:\n",
      "input: 1, 1, 1 | output = [[0.9999]]\n"
     ]
    }
   ],
   "source": [
    "compute_neural_network(0,0,1)\n",
    "compute_neural_network(1,1,1)\n",
    "compute_neural_network(1,0,1)\n",
    "compute_neural_network(0,1,1)\n",
    "println(\"Prediction:\")\n",
    "compute_neural_network(1,1,1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kotlin",
   "language": "kotlin",
   "name": "kotlin"
  },
  "language_info": {
   "codemirror_mode": "text/x-kotlin",
   "file_extension": ".kt",
   "mimetype": "text/x-kotlin",
   "name": "kotlin",
   "pygments_lexer": "kotlin",
   "version": "1.4.0-dev-7568"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
